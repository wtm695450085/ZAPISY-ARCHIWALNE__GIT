{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch_12_ Prosty przyk≈Çad \n",
    "https://medium.com/coinmonks/create-a-neural-network-in-pytorch-and-make-your-life-simpler-ec5367895199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input size, hidden layer size, output size and batch size respectively\n",
    "n_in, n_h, n_out, batch_size = 5, 5, 1, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input and target tensors (data)\n",
    "x = torch.tensor([[-1.2,  0.9,  0.7, -1.1,  0.1],[ 0.5,  0.2,  0.5, -1.3,  0.6],[-0.9, -0.8,  0.5, -0.5,  0.7],[-0.4,  1.3, -1.7, -0.7, -2.1],[-0.3,  0.9,  2.7,  0.6, -1.4]], dtype=torch.float)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([5, 5])\n",
      "y: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print('x:',x.shape)\n",
    "print('y:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2000,  0.9000,  0.7000, -1.1000,  0.1000],\n",
       "        [ 0.5000,  0.2000,  0.5000, -1.3000,  0.6000],\n",
       "        [-0.9000, -0.8000,  0.5000, -0.5000,  0.7000],\n",
       "        [-0.4000,  1.3000, -1.7000, -0.7000, -2.1000],\n",
       "        [-0.3000,  0.9000,  2.7000,  0.6000, -1.4000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.27935534715652466\n",
      "epoch:  1  loss:  0.27898991107940674\n",
      "epoch:  2  loss:  0.27862682938575745\n",
      "epoch:  3  loss:  0.27826595306396484\n",
      "epoch:  4  loss:  0.2779074013233185\n",
      "epoch:  5  loss:  0.2775510847568512\n",
      "epoch:  6  loss:  0.2771969437599182\n",
      "epoch:  7  loss:  0.2768450677394867\n",
      "epoch:  8  loss:  0.2764953672885895\n",
      "epoch:  9  loss:  0.2761478126049042\n",
      "epoch:  10  loss:  0.27580246329307556\n",
      "epoch:  11  loss:  0.2754592001438141\n",
      "epoch:  12  loss:  0.2751180827617645\n",
      "epoch:  13  loss:  0.2747790515422821\n",
      "epoch:  14  loss:  0.2744421064853668\n",
      "epoch:  15  loss:  0.2741071879863739\n",
      "epoch:  16  loss:  0.27377432584762573\n",
      "epoch:  17  loss:  0.2734435498714447\n",
      "epoch:  18  loss:  0.27311471104621887\n",
      "epoch:  19  loss:  0.2727878987789154\n",
      "epoch:  20  loss:  0.2724630534648895\n",
      "epoch:  21  loss:  0.27214017510414124\n",
      "epoch:  22  loss:  0.27181923389434814\n",
      "epoch:  23  loss:  0.27150020003318787\n",
      "epoch:  24  loss:  0.2711831331253052\n",
      "epoch:  25  loss:  0.27086788415908813\n",
      "epoch:  26  loss:  0.2705545723438263\n",
      "epoch:  27  loss:  0.2702430784702301\n",
      "epoch:  28  loss:  0.26993340253829956\n",
      "epoch:  29  loss:  0.26962560415267944\n",
      "epoch:  30  loss:  0.2693195939064026\n",
      "epoch:  31  loss:  0.2690154016017914\n",
      "epoch:  32  loss:  0.26871296763420105\n",
      "epoch:  33  loss:  0.2684122920036316\n",
      "epoch:  34  loss:  0.2681133449077606\n",
      "epoch:  35  loss:  0.2678161859512329\n",
      "epoch:  36  loss:  0.2675207257270813\n",
      "epoch:  37  loss:  0.2672269940376282\n",
      "epoch:  38  loss:  0.266934871673584\n",
      "epoch:  39  loss:  0.2666444778442383\n",
      "epoch:  40  loss:  0.2663557231426239\n",
      "epoch:  41  loss:  0.26606863737106323\n",
      "epoch:  42  loss:  0.2657831311225891\n",
      "epoch:  43  loss:  0.2654992938041687\n",
      "epoch:  44  loss:  0.26521700620651245\n",
      "epoch:  45  loss:  0.2649363577365875\n",
      "epoch:  46  loss:  0.26465725898742676\n",
      "epoch:  47  loss:  0.26437970995903015\n",
      "epoch:  48  loss:  0.2641037106513977\n",
      "epoch:  49  loss:  0.26382920145988464\n",
      "epoch:  50  loss:  0.2635563015937805\n",
      "epoch:  51  loss:  0.263284832239151\n",
      "epoch:  52  loss:  0.26301485300064087\n",
      "epoch:  53  loss:  0.2627463638782501\n",
      "epoch:  54  loss:  0.26247936487197876\n",
      "epoch:  55  loss:  0.2622137665748596\n",
      "epoch:  56  loss:  0.2619495987892151\n",
      "epoch:  57  loss:  0.26168692111968994\n",
      "epoch:  58  loss:  0.26142561435699463\n",
      "epoch:  59  loss:  0.26116570830345154\n",
      "epoch:  60  loss:  0.26090720295906067\n",
      "epoch:  61  loss:  0.26065003871917725\n",
      "epoch:  62  loss:  0.26039424538612366\n",
      "epoch:  63  loss:  0.2601397931575775\n",
      "epoch:  64  loss:  0.2598866820335388\n",
      "epoch:  65  loss:  0.25963491201400757\n",
      "epoch:  66  loss:  0.2593844532966614\n",
      "epoch:  67  loss:  0.25913530588150024\n",
      "epoch:  68  loss:  0.258887380361557\n",
      "epoch:  69  loss:  0.2586408257484436\n",
      "epoch:  70  loss:  0.2583954930305481\n",
      "epoch:  71  loss:  0.25815141201019287\n",
      "epoch:  72  loss:  0.25790855288505554\n",
      "epoch:  73  loss:  0.2576669454574585\n",
      "epoch:  74  loss:  0.25742655992507935\n",
      "epoch:  75  loss:  0.2571874260902405\n",
      "epoch:  76  loss:  0.25694939494132996\n",
      "epoch:  77  loss:  0.2567126154899597\n",
      "epoch:  78  loss:  0.2564769983291626\n",
      "epoch:  79  loss:  0.256242573261261\n",
      "epoch:  80  loss:  0.2560092508792877\n",
      "epoch:  81  loss:  0.2557770907878876\n",
      "epoch:  82  loss:  0.25554606318473816\n",
      "epoch:  83  loss:  0.2553161680698395\n",
      "epoch:  84  loss:  0.25508734583854675\n",
      "epoch:  85  loss:  0.25485965609550476\n",
      "epoch:  86  loss:  0.2546330988407135\n",
      "epoch:  87  loss:  0.2544075846672058\n",
      "epoch:  88  loss:  0.2541831135749817\n",
      "epoch:  89  loss:  0.2539597451686859\n",
      "epoch:  90  loss:  0.2537374198436737\n",
      "epoch:  91  loss:  0.25351613759994507\n",
      "epoch:  92  loss:  0.2532958984375\n",
      "epoch:  93  loss:  0.2530766725540161\n",
      "epoch:  94  loss:  0.2528584599494934\n",
      "epoch:  95  loss:  0.2526412606239319\n",
      "epoch:  96  loss:  0.25242501497268677\n",
      "epoch:  97  loss:  0.2522098124027252\n",
      "epoch:  98  loss:  0.2519955337047577\n",
      "epoch:  99  loss:  0.25178223848342896\n",
      "epoch:  100  loss:  0.251569926738739\n",
      "epoch:  101  loss:  0.2513585388660431\n",
      "epoch:  102  loss:  0.2511481046676636\n",
      "epoch:  103  loss:  0.2509385943412781\n",
      "epoch:  104  loss:  0.2507300078868866\n",
      "epoch:  105  loss:  0.25052231550216675\n",
      "epoch:  106  loss:  0.2503156065940857\n",
      "epoch:  107  loss:  0.2501097023487091\n",
      "epoch:  108  loss:  0.24990472197532654\n",
      "epoch:  109  loss:  0.249700665473938\n",
      "epoch:  110  loss:  0.24949738383293152\n",
      "epoch:  111  loss:  0.24929502606391907\n",
      "epoch:  112  loss:  0.24909348785877228\n",
      "epoch:  113  loss:  0.2488928735256195\n",
      "epoch:  114  loss:  0.24869301915168762\n",
      "epoch:  115  loss:  0.24849405884742737\n",
      "epoch:  116  loss:  0.2482958734035492\n",
      "epoch:  117  loss:  0.24809852242469788\n",
      "epoch:  118  loss:  0.2479020059108734\n",
      "epoch:  119  loss:  0.24770624935626984\n",
      "epoch:  120  loss:  0.24751129746437073\n",
      "epoch:  121  loss:  0.2473171204328537\n",
      "epoch:  122  loss:  0.24712371826171875\n",
      "epoch:  123  loss:  0.24693112075328827\n",
      "epoch:  124  loss:  0.2467392385005951\n",
      "epoch:  125  loss:  0.246548131108284\n",
      "epoch:  126  loss:  0.2463577687740326\n",
      "epoch:  127  loss:  0.24616818130016327\n",
      "epoch:  128  loss:  0.24597927927970886\n",
      "epoch:  129  loss:  0.24579115211963654\n",
      "epoch:  130  loss:  0.24560371041297913\n",
      "epoch:  131  loss:  0.24541696906089783\n",
      "epoch:  132  loss:  0.2452310025691986\n",
      "epoch:  133  loss:  0.24504566192626953\n",
      "epoch:  134  loss:  0.24486105144023895\n",
      "epoch:  135  loss:  0.2446771115064621\n",
      "epoch:  136  loss:  0.24449381232261658\n",
      "epoch:  137  loss:  0.24431125819683075\n",
      "epoch:  138  loss:  0.24412934482097626\n",
      "epoch:  139  loss:  0.2439480572938919\n",
      "epoch:  140  loss:  0.24376742541790009\n",
      "epoch:  141  loss:  0.24358749389648438\n",
      "epoch:  142  loss:  0.24340815842151642\n",
      "epoch:  143  loss:  0.2432294636964798\n",
      "epoch:  144  loss:  0.24305137991905212\n",
      "epoch:  145  loss:  0.2428739070892334\n",
      "epoch:  146  loss:  0.2426971197128296\n",
      "epoch:  147  loss:  0.24252089858055115\n",
      "epoch:  148  loss:  0.24234528839588165\n",
      "epoch:  149  loss:  0.24217024445533752\n",
      "epoch:  150  loss:  0.24199581146240234\n",
      "epoch:  151  loss:  0.24182197451591492\n",
      "epoch:  152  loss:  0.24164867401123047\n",
      "epoch:  153  loss:  0.24147598445415497\n",
      "epoch:  154  loss:  0.24130387604236603\n",
      "epoch:  155  loss:  0.24113230407238007\n",
      "epoch:  156  loss:  0.24097351729869843\n",
      "epoch:  157  loss:  0.24081838130950928\n",
      "epoch:  158  loss:  0.24066372215747833\n",
      "epoch:  159  loss:  0.2405095398426056\n",
      "epoch:  160  loss:  0.24035584926605225\n",
      "epoch:  161  loss:  0.24020257592201233\n",
      "epoch:  162  loss:  0.2400498390197754\n",
      "epoch:  163  loss:  0.2398974597454071\n",
      "epoch:  164  loss:  0.2397456169128418\n",
      "epoch:  165  loss:  0.23959414660930634\n",
      "epoch:  166  loss:  0.23944315314292908\n",
      "epoch:  167  loss:  0.23929259181022644\n",
      "epoch:  168  loss:  0.2391424924135208\n",
      "epoch:  169  loss:  0.23899276554584503\n",
      "epoch:  170  loss:  0.23884351551532745\n",
      "epoch:  171  loss:  0.23869463801383972\n",
      "epoch:  172  loss:  0.23854617774486542\n",
      "epoch:  173  loss:  0.23839811980724335\n",
      "epoch:  174  loss:  0.2382504642009735\n",
      "epoch:  175  loss:  0.2381032258272171\n",
      "epoch:  176  loss:  0.23795640468597412\n",
      "epoch:  177  loss:  0.2378098964691162\n",
      "epoch:  178  loss:  0.23766383528709412\n",
      "epoch:  179  loss:  0.23751811683177948\n",
      "epoch:  180  loss:  0.23737280070781708\n",
      "epoch:  181  loss:  0.23722782731056213\n",
      "epoch:  182  loss:  0.23708324134349823\n",
      "epoch:  183  loss:  0.23693902790546417\n",
      "epoch:  184  loss:  0.23679514229297638\n",
      "epoch:  185  loss:  0.23665161430835724\n",
      "epoch:  186  loss:  0.23650845885276794\n",
      "epoch:  187  loss:  0.23636563122272491\n",
      "epoch:  188  loss:  0.23622314631938934\n",
      "epoch:  189  loss:  0.23608103394508362\n",
      "epoch:  190  loss:  0.23593921959400177\n",
      "epoch:  191  loss:  0.235797718167305\n",
      "epoch:  192  loss:  0.23565658926963806\n",
      "epoch:  193  loss:  0.2355157434940338\n",
      "epoch:  194  loss:  0.23537524044513702\n",
      "epoch:  195  loss:  0.23523502051830292\n",
      "epoch:  196  loss:  0.23509511351585388\n",
      "epoch:  197  loss:  0.2349555492401123\n",
      "epoch:  198  loss:  0.2348162829875946\n",
      "epoch:  199  loss:  0.2346772849559784\n",
      "epoch:  200  loss:  0.23453859984874725\n",
      "epoch:  201  loss:  0.23440022766590118\n",
      "epoch:  202  loss:  0.2342621088027954\n",
      "epoch:  203  loss:  0.2341243028640747\n",
      "epoch:  204  loss:  0.2339867353439331\n",
      "epoch:  205  loss:  0.23384949564933777\n",
      "epoch:  206  loss:  0.23371247947216034\n",
      "epoch:  207  loss:  0.2335757464170456\n",
      "epoch:  208  loss:  0.23343929648399353\n",
      "epoch:  209  loss:  0.23330311477184296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  210  loss:  0.23316717147827148\n",
      "epoch:  211  loss:  0.2330314815044403\n",
      "epoch:  212  loss:  0.232896089553833\n",
      "epoch:  213  loss:  0.23276090621948242\n",
      "epoch:  214  loss:  0.23262596130371094\n",
      "epoch:  215  loss:  0.23249125480651855\n",
      "epoch:  216  loss:  0.23235681653022766\n",
      "epoch:  217  loss:  0.23222915828227997\n",
      "epoch:  218  loss:  0.23210719227790833\n",
      "epoch:  219  loss:  0.23198536038398743\n",
      "epoch:  220  loss:  0.23186369240283966\n",
      "epoch:  221  loss:  0.23174205422401428\n",
      "epoch:  222  loss:  0.23162052035331726\n",
      "epoch:  223  loss:  0.23149915039539337\n",
      "epoch:  224  loss:  0.23137788474559784\n",
      "epoch:  225  loss:  0.23125672340393066\n",
      "epoch:  226  loss:  0.23113560676574707\n",
      "epoch:  227  loss:  0.23101463913917542\n",
      "epoch:  228  loss:  0.23089376091957092\n",
      "epoch:  229  loss:  0.2307729721069336\n",
      "epoch:  230  loss:  0.23065230250358582\n",
      "epoch:  231  loss:  0.2305317372083664\n",
      "epoch:  232  loss:  0.23041124641895294\n",
      "epoch:  233  loss:  0.23029084503650665\n",
      "epoch:  234  loss:  0.23017048835754395\n",
      "epoch:  235  loss:  0.23005028069019318\n",
      "epoch:  236  loss:  0.22993013262748718\n",
      "epoch:  237  loss:  0.22981004416942596\n",
      "epoch:  238  loss:  0.22969010472297668\n",
      "epoch:  239  loss:  0.2295701950788498\n",
      "epoch:  240  loss:  0.22945037484169006\n",
      "epoch:  241  loss:  0.2293306589126587\n",
      "epoch:  242  loss:  0.2292110025882721\n",
      "epoch:  243  loss:  0.2290913611650467\n",
      "epoch:  244  loss:  0.22897186875343323\n",
      "epoch:  245  loss:  0.22885242104530334\n",
      "epoch:  246  loss:  0.22873303294181824\n",
      "epoch:  247  loss:  0.2286137044429779\n",
      "epoch:  248  loss:  0.22849449515342712\n",
      "epoch:  249  loss:  0.22837531566619873\n",
      "epoch:  250  loss:  0.2282561957836151\n",
      "epoch:  251  loss:  0.22813716530799866\n",
      "epoch:  252  loss:  0.2280181348323822\n",
      "epoch:  253  loss:  0.2278992235660553\n",
      "epoch:  254  loss:  0.22778034210205078\n",
      "epoch:  255  loss:  0.22766152024269104\n",
      "epoch:  256  loss:  0.22754275798797607\n",
      "epoch:  257  loss:  0.22742405533790588\n",
      "epoch:  258  loss:  0.2273053675889969\n",
      "epoch:  259  loss:  0.22718676924705505\n",
      "epoch:  260  loss:  0.22706818580627441\n",
      "epoch:  261  loss:  0.22694972157478333\n",
      "epoch:  262  loss:  0.22683122754096985\n",
      "epoch:  263  loss:  0.22671279311180115\n",
      "epoch:  264  loss:  0.22659440338611603\n",
      "epoch:  265  loss:  0.22647607326507568\n",
      "epoch:  266  loss:  0.22635777294635773\n",
      "epoch:  267  loss:  0.22623948752880096\n",
      "epoch:  268  loss:  0.22612126171588898\n",
      "epoch:  269  loss:  0.22600308060646057\n",
      "epoch:  270  loss:  0.22588488459587097\n",
      "epoch:  271  loss:  0.22576680779457092\n",
      "epoch:  272  loss:  0.22564871609210968\n",
      "epoch:  273  loss:  0.22553066909313202\n",
      "epoch:  274  loss:  0.2254091203212738\n",
      "epoch:  275  loss:  0.2252720296382904\n",
      "epoch:  276  loss:  0.22513481974601746\n",
      "epoch:  277  loss:  0.22499749064445496\n",
      "epoch:  278  loss:  0.22485999763011932\n",
      "epoch:  279  loss:  0.22472241520881653\n",
      "epoch:  280  loss:  0.2245846688747406\n",
      "epoch:  281  loss:  0.22444677352905273\n",
      "epoch:  282  loss:  0.2243087738752365\n",
      "epoch:  283  loss:  0.22417061030864716\n",
      "epoch:  284  loss:  0.22403235733509064\n",
      "epoch:  285  loss:  0.2238939106464386\n",
      "epoch:  286  loss:  0.22375532984733582\n",
      "epoch:  287  loss:  0.2236166000366211\n",
      "epoch:  288  loss:  0.22347775101661682\n",
      "epoch:  289  loss:  0.22333872318267822\n",
      "epoch:  290  loss:  0.2231995314359665\n",
      "epoch:  291  loss:  0.22306020557880402\n",
      "epoch:  292  loss:  0.2229207456111908\n",
      "epoch:  293  loss:  0.22278109192848206\n",
      "epoch:  294  loss:  0.22264131903648376\n",
      "epoch:  295  loss:  0.22250135242938995\n",
      "epoch:  296  loss:  0.2223612368106842\n",
      "epoch:  297  loss:  0.22222092747688293\n",
      "epoch:  298  loss:  0.22208046913146973\n",
      "epoch:  299  loss:  0.2219398468732834\n",
      "epoch:  300  loss:  0.2217990607023239\n",
      "epoch:  301  loss:  0.2216581106185913\n",
      "epoch:  302  loss:  0.22151696681976318\n",
      "epoch:  303  loss:  0.22137565910816193\n",
      "epoch:  304  loss:  0.22123417258262634\n",
      "epoch:  305  loss:  0.22109250724315643\n",
      "epoch:  306  loss:  0.2209506779909134\n",
      "epoch:  307  loss:  0.22080865502357483\n",
      "epoch:  308  loss:  0.22066640853881836\n",
      "epoch:  309  loss:  0.22052404284477234\n",
      "epoch:  310  loss:  0.22038142383098602\n",
      "epoch:  311  loss:  0.22023865580558777\n",
      "epoch:  312  loss:  0.2200956791639328\n",
      "epoch:  313  loss:  0.2199525386095047\n",
      "epoch:  314  loss:  0.21980920433998108\n",
      "epoch:  315  loss:  0.21966561675071716\n",
      "epoch:  316  loss:  0.2195218801498413\n",
      "epoch:  317  loss:  0.21937792003154755\n",
      "epoch:  318  loss:  0.21923379600048065\n",
      "epoch:  319  loss:  0.21908941864967346\n",
      "epoch:  320  loss:  0.21894486248493195\n",
      "epoch:  321  loss:  0.21880009770393372\n",
      "epoch:  322  loss:  0.21865513920783997\n",
      "epoch:  323  loss:  0.21850994229316711\n",
      "epoch:  324  loss:  0.21836456656455994\n",
      "epoch:  325  loss:  0.21821896731853485\n",
      "epoch:  326  loss:  0.21807312965393066\n",
      "epoch:  327  loss:  0.21792712807655334\n",
      "epoch:  328  loss:  0.21778090298175812\n",
      "epoch:  329  loss:  0.2176344096660614\n",
      "epoch:  330  loss:  0.21748772263526917\n",
      "epoch:  331  loss:  0.21734079718589783\n",
      "epoch:  332  loss:  0.21719364821910858\n",
      "epoch:  333  loss:  0.2170463353395462\n",
      "epoch:  334  loss:  0.21689870953559875\n",
      "epoch:  335  loss:  0.2167508900165558\n",
      "epoch:  336  loss:  0.2166028767824173\n",
      "epoch:  337  loss:  0.21645458042621613\n",
      "epoch:  338  loss:  0.21630609035491943\n",
      "epoch:  339  loss:  0.21615734696388245\n",
      "epoch:  340  loss:  0.21600838005542755\n",
      "epoch:  341  loss:  0.21585914492607117\n",
      "epoch:  342  loss:  0.21570971608161926\n",
      "epoch:  343  loss:  0.21556000411510468\n",
      "epoch:  344  loss:  0.21541006863117218\n",
      "epoch:  345  loss:  0.21525990962982178\n",
      "epoch:  346  loss:  0.21510949730873108\n",
      "epoch:  347  loss:  0.21495886147022247\n",
      "epoch:  348  loss:  0.21480794250965118\n",
      "epoch:  349  loss:  0.2146567553281784\n",
      "epoch:  350  loss:  0.2145053595304489\n",
      "epoch:  351  loss:  0.21435365080833435\n",
      "epoch:  352  loss:  0.21420177817344666\n",
      "epoch:  353  loss:  0.2140495777130127\n",
      "epoch:  354  loss:  0.21389718353748322\n",
      "epoch:  355  loss:  0.21374449133872986\n",
      "epoch:  356  loss:  0.2135915756225586\n",
      "epoch:  357  loss:  0.21343836188316345\n",
      "epoch:  358  loss:  0.2132849246263504\n",
      "epoch:  359  loss:  0.21313121914863586\n",
      "epoch:  360  loss:  0.21297721564769745\n",
      "epoch:  361  loss:  0.21282295882701874\n",
      "epoch:  362  loss:  0.21266846358776093\n",
      "epoch:  363  loss:  0.21251368522644043\n",
      "epoch:  364  loss:  0.21235862374305725\n",
      "epoch:  365  loss:  0.21220330893993378\n",
      "epoch:  366  loss:  0.2120477259159088\n",
      "epoch:  367  loss:  0.21189184486865997\n",
      "epoch:  368  loss:  0.21173569560050964\n",
      "epoch:  369  loss:  0.2115793228149414\n",
      "epoch:  370  loss:  0.2114226371049881\n",
      "epoch:  371  loss:  0.2112656533718109\n",
      "epoch:  372  loss:  0.2111084759235382\n",
      "epoch:  373  loss:  0.21095092594623566\n",
      "epoch:  374  loss:  0.210793137550354\n",
      "epoch:  375  loss:  0.21063503623008728\n",
      "epoch:  376  loss:  0.21047668159008026\n",
      "epoch:  377  loss:  0.21031804382801056\n",
      "epoch:  378  loss:  0.21015910804271698\n",
      "epoch:  379  loss:  0.20999988913536072\n",
      "epoch:  380  loss:  0.20984038710594177\n",
      "epoch:  381  loss:  0.20968060195446014\n",
      "epoch:  382  loss:  0.20952053368091583\n",
      "epoch:  383  loss:  0.20936015248298645\n",
      "epoch:  384  loss:  0.2091994732618332\n",
      "epoch:  385  loss:  0.20903854072093964\n",
      "epoch:  386  loss:  0.20887728035449982\n",
      "epoch:  387  loss:  0.20871572196483612\n",
      "epoch:  388  loss:  0.20855388045310974\n",
      "epoch:  389  loss:  0.20839175581932068\n",
      "epoch:  390  loss:  0.20822933316230774\n",
      "epoch:  391  loss:  0.20806658267974854\n",
      "epoch:  392  loss:  0.20790354907512665\n",
      "epoch:  393  loss:  0.20774023234844208\n",
      "epoch:  394  loss:  0.20757658779621124\n",
      "epoch:  395  loss:  0.20741264522075653\n",
      "epoch:  396  loss:  0.20724841952323914\n",
      "epoch:  397  loss:  0.20708385109901428\n",
      "epoch:  398  loss:  0.20691902935504913\n",
      "epoch:  399  loss:  0.20675384998321533\n",
      "epoch:  400  loss:  0.20658835768699646\n",
      "epoch:  401  loss:  0.2064226120710373\n",
      "epoch:  402  loss:  0.20625653862953186\n",
      "epoch:  403  loss:  0.20609013736248016\n",
      "epoch:  404  loss:  0.20592346787452698\n",
      "epoch:  405  loss:  0.20575642585754395\n",
      "epoch:  406  loss:  0.20558910071849823\n",
      "epoch:  407  loss:  0.20542144775390625\n",
      "epoch:  408  loss:  0.20525352656841278\n",
      "epoch:  409  loss:  0.20508523285388947\n",
      "epoch:  410  loss:  0.20491667091846466\n",
      "epoch:  411  loss:  0.2047477662563324\n",
      "epoch:  412  loss:  0.20457854866981506\n",
      "epoch:  413  loss:  0.20440900325775146\n",
      "epoch:  414  loss:  0.20423917472362518\n",
      "epoch:  415  loss:  0.20406898856163025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  416  loss:  0.20389850437641144\n",
      "epoch:  417  loss:  0.20372769236564636\n",
      "epoch:  418  loss:  0.2035565823316574\n",
      "epoch:  419  loss:  0.2033851146697998\n",
      "epoch:  420  loss:  0.20321330428123474\n",
      "epoch:  421  loss:  0.2030412256717682\n",
      "epoch:  422  loss:  0.20286878943443298\n",
      "epoch:  423  loss:  0.2026960402727127\n",
      "epoch:  424  loss:  0.20252296328544617\n",
      "epoch:  425  loss:  0.20234954357147217\n",
      "epoch:  426  loss:  0.2021758258342743\n",
      "epoch:  427  loss:  0.20200176537036896\n",
      "epoch:  428  loss:  0.20182737708091736\n",
      "epoch:  429  loss:  0.2016526758670807\n",
      "epoch:  430  loss:  0.20147764682769775\n",
      "epoch:  431  loss:  0.20130228996276855\n",
      "epoch:  432  loss:  0.20112654566764832\n",
      "epoch:  433  loss:  0.2009505331516266\n",
      "epoch:  434  loss:  0.2007741630077362\n",
      "epoch:  435  loss:  0.20059747993946075\n",
      "epoch:  436  loss:  0.20042045414447784\n",
      "epoch:  437  loss:  0.20024311542510986\n",
      "epoch:  438  loss:  0.20006541907787323\n",
      "epoch:  439  loss:  0.19988739490509033\n",
      "epoch:  440  loss:  0.19970904290676117\n",
      "epoch:  441  loss:  0.19953033328056335\n",
      "epoch:  442  loss:  0.19935132563114166\n",
      "epoch:  443  loss:  0.1991719752550125\n",
      "epoch:  444  loss:  0.1989922821521759\n",
      "epoch:  445  loss:  0.19881221652030945\n",
      "epoch:  446  loss:  0.1986318677663803\n",
      "epoch:  447  loss:  0.19845116138458252\n",
      "epoch:  448  loss:  0.19827014207839966\n",
      "epoch:  449  loss:  0.19808875024318695\n",
      "epoch:  450  loss:  0.19790706038475037\n",
      "epoch:  451  loss:  0.19772499799728394\n",
      "epoch:  452  loss:  0.19754262268543243\n",
      "epoch:  453  loss:  0.1973598748445511\n",
      "epoch:  454  loss:  0.19717681407928467\n",
      "epoch:  455  loss:  0.19699342548847198\n",
      "epoch:  456  loss:  0.19680967926979065\n",
      "epoch:  457  loss:  0.19662559032440186\n",
      "epoch:  458  loss:  0.19644120335578918\n",
      "epoch:  459  loss:  0.19625644385814667\n",
      "epoch:  460  loss:  0.19607135653495789\n",
      "epoch:  461  loss:  0.19588591158390045\n",
      "epoch:  462  loss:  0.19570012390613556\n",
      "epoch:  463  loss:  0.1955140382051468\n",
      "epoch:  464  loss:  0.19532756507396698\n",
      "epoch:  465  loss:  0.1951407939195633\n",
      "epoch:  466  loss:  0.19495365023612976\n",
      "epoch:  467  loss:  0.19476616382598877\n",
      "epoch:  468  loss:  0.1945783495903015\n",
      "epoch:  469  loss:  0.1943902224302292\n",
      "epoch:  470  loss:  0.19420169293880463\n",
      "epoch:  471  loss:  0.19401288032531738\n",
      "epoch:  472  loss:  0.1938237100839615\n",
      "epoch:  473  loss:  0.19363419711589813\n",
      "epoch:  474  loss:  0.19344434142112732\n",
      "epoch:  475  loss:  0.19325414299964905\n",
      "epoch:  476  loss:  0.19306360185146332\n",
      "epoch:  477  loss:  0.1928727626800537\n",
      "epoch:  478  loss:  0.19268155097961426\n",
      "epoch:  479  loss:  0.19249001145362854\n",
      "epoch:  480  loss:  0.19229811429977417\n",
      "epoch:  481  loss:  0.19210587441921234\n",
      "epoch:  482  loss:  0.19191330671310425\n",
      "epoch:  483  loss:  0.19172042608261108\n",
      "epoch:  484  loss:  0.19152717292308807\n",
      "epoch:  485  loss:  0.19133360683918\n",
      "epoch:  486  loss:  0.19113966822624207\n",
      "epoch:  487  loss:  0.19094541668891907\n",
      "epoch:  488  loss:  0.19075080752372742\n",
      "epoch:  489  loss:  0.1905559003353119\n",
      "epoch:  490  loss:  0.1903606355190277\n",
      "epoch:  491  loss:  0.19016501307487488\n",
      "epoch:  492  loss:  0.1899690479040146\n",
      "epoch:  493  loss:  0.18977276980876923\n",
      "epoch:  494  loss:  0.1895761787891388\n",
      "epoch:  495  loss:  0.18937921524047852\n",
      "epoch:  496  loss:  0.18918193876743317\n",
      "epoch:  497  loss:  0.18898431956768036\n",
      "epoch:  498  loss:  0.1887863576412201\n",
      "epoch:  499  loss:  0.18858808279037476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
